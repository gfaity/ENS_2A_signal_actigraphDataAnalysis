---
title: "Standardized Activity Analysis"
format:
  html:
    self-contained: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE
)
#.vsc.attach()
```


```{r}
#| echo: true
#| code-fold: false

# libraries
library(dplyr) # manipulation de dataframes
library(tidyr) # opération sur les dataframes
library(ggplot2) # générer des illustations graphiques
library(seewave) # pour les fonctions de filtrage
library(signal) # pour certaines manipulations dans le domaine frequentiel
library(patchwork) # pour arranger les plots facilement
library(read.gt3x) #check also GGIR (for reading gt3x files [fast using C++])
library(caret)    # Pour la sélection et la normalisation des features
library(RcppRoll)  # Pour calculer les distances glissantes (utilitaire)
library(data.table) # faster than data.frame

# WRK_PATH est le chemin vers le répertoire où se trouve notre projet et où se trouvent les fichiers mocap.
# Vous devrez ajuster ce chemin en fonction de votre propre répertoire.
WRK_PATH <- "C:/Users/germa/OneDrive/Documents/GitHub/ENS_2A_signal_actigraphDataAnalysis/actigraphDataAnalysis"

# La fonction `setwd()` change le répertoire de travail pour celui que nous avons défini dans WRK_PATH.
# Cela signifie que toute lecture/écriture de fichiers se fera à partir de ce répertoire.
setwd(WRK_PATH)

# On utilise la fonction `getwd()` pour afficher le répertoire de travail actuel et vérifier que le changement a bien eu lieu.
cat("Répertoire de travail actuel :", getwd(), "\n")

# Ensuite, nous définissons le chemin vers le dossier contenant les données à traiter et le chemin pour les résultats.
# Ce dossier se trouve à l'intérieur de notre répertoire de travail.
DAT_PATH <- file.path(WRK_PATH, "data")
FCT_PATH <- file.path(WRK_PATH, "FCT", "R") #dossier contenant les fonctions R
RES_PATH <- file.path(WRK_PATH, "RES")

# Dans quel cas somme nous ?
thisCase <- "TP2_APstandard" # "TP1_freeliving" 
thisDAT_PATH <- file.path(DAT_PATH, thisCase)
thisRES_PATH <- file.path(RES_PATH, thisCase)

# On affiche également le chemin vers les données pour s'assurer qu'il est correct.
cat("Répertoire des données :", thisDAT_PATH, "\n")

# Lire les fonctions R
r_files <- list.files(FCT_PATH, pattern = "\\.R$", full.names = TRUE)
# Charger chaque fichier .R sans afficher de sortie
invisible(lapply(r_files, source))
# Afficher le nombre de fichiers .R chargés
cat(length(r_files), "fichiers .R chargés avec succès.\n")
```

Nous sommes ici dans le cas `TP2 - activités standardisées`.

Le but de ce code est le suivant:
    - aller chercher les fichiers gt3x des activités standardisées (1 fichier par participant)
    - aller chercher le fichier de labellisation (1 fichier global pour l'ensemble de la classe) pour obtenir les dates-heures de début et fin de chaque activité
    - transformer les signaux de base pour obtenir 1 signal unique (norme de l'accélération) et propre (filtré, lissé)
    - segmenter les data et labelliser en fonction des activités standardisées en cours
    - pour chaque activité standardisé, extraire des paramètres d'intérêt (features extraction)
    - ensuite, faire une sélection des paramètres les plus pertinents (feature selection)
    - finalement, extraire tout cela dans un fichier .csv pour traitement futur (machine learning pour activity recognition)

Commençons par lister les fichiers à aller chercher.

```{r}
# Lire le fichier contenant les informations de label
DFlabel <- read.csv2(file.path(DAT_PATH, "2A_2024_hoursActivity.csv"))
# Nettoyer le fichier de label
DFlabel <- cleanDFlabel(DFlabel)

# Lister les fichiers gt3x de tous les participants
gt3x_files <- list.files(thisDAT_PATH, pattern = "*.gt3x", full.names = TRUE)
```

Maintenant rentrons dans le vif du sujet: faire une boucle pour tous les participants.

```{r}
# Set some parameters before looping

# Window size and overlap
window_size_sec <- 5 #10 #5 #2    # 2 or 5-second windows
overlap_fraction <- 0.5 # 50% overlap
# Cutting frequency for filtering with butterworth
cut_freq <- 10 #no human movement above 10Hz

# Pour enregistrer nos resultats
windowSizeStr <- paste("windowSize_", window_size_sec, "s", sep="")
dir.create(file.path(thisRES_PATH, windowSizeStr), recursive = TRUE) #créer le dossier si pas déjà existant

# Création d'une liste vide pour stocker les informations pour chaque participant
participant_data_list <- list()

# Loop over each participant's GT3X file
for (file_path in gt3x_files) {

    # Extract participant ID from the file name
    # Each file name is like this: "IDPARTICIPANT (AAAA-MM-DD).gt3x"
    participant_fileName <- sub("\\.gt3x$", "", basename(file_path))
    # Utiliser strsplit avec les caractères spéciaux échappés
    resultStr <- strsplit(participant_fileName, "[() ]", perl = TRUE)
    participant_id <- resultStr[[1]][1]
    date <- resultStr[[1]][3]

    # Print status
    cat("Processing Participant:", participant_id, "- date start recording:",date, "\n")

    # Import Accelerometer Data
    # Use GGIR or an appropriate package to read GT3X files
    thisDF <- read.gt3x(file_path)
    # Clean data and get sampling rate
    thisDF <- as.data.frame(thisDF)
    sampling_rate <- attributes(thisDF)$sample_rate
    # Filter data
    thisDF <- as.data.frame(cleanFilterAccelero(as.data.table(thisDF), cut_freq))
    #  Compute accelerometer norm
    thisDF <- as.data.frame(getEMNO(as.data.table(thisDF)))
    
    # Import and Assign Activity Labels for the Participant
    participant_labels <- subset(DFlabel, ID==participant_id)

    # Skip if no labels are available
    if (nrow(participant_labels) == 0) {
        next
    }

    # Initialize a list to store features for this participant
    participant_features <- list()

    # Loop over each activity phase
    for (i in seq_len(nrow(participant_labels))) {
        # Extract activity phase details
        current_activity <- participant_labels$activity[i]
        current_phase <- participant_labels$phase[i]
        if(is.na(current_phase)){current_phase = 1}
        start_time <- participant_labels$start[i]
        end_time <- participant_labels$end[i]
        
        # Subset data for this activity phase
        activity_data <- subset(thisDF, time >= start_time & time <= end_time)

        # Skip if no data for this phase
        if (nrow(activity_data) == 0) {
            next
        }

        # Segment Data into Windows
        activity_data <- segmentDataIntoWindows(activity_data, sampling_rate, window_size_sec, overlap_fraction)

        # Initialiser une liste pour stocker les caractéristiques de cette phase d'activité
        phase_features <- list()

        # Obtenir les identifiants uniques des fenêtres
        window_ids <- unique(activity_data$window)

        # Boucler sur chaque fenêtre
        for (w in window_ids) {
            # Sous-ensemble des données pour la fenêtre actuelle
            window_data <- subset(activity_data, window == w)

            # Extraire les caractéristiques de la fenêtre
            features <- extract_features(window_data, sampling_rate)
            
            # Vérifier si l'extraction des caractéristiques a réussi
            if (is.null(features)) {
                next  # Passer à la fenêtre suivante si aucune caractéristique n'a été extraite
            }
            
            # Ajouter les étiquettes aux caractéristiques
            features$participant_id <- participant_id
            features$activity <- current_activity
            features$phase <- current_phase
            features$window_id <- as.numeric(w)
            features$start_time <- window_data$time[1]
            features$end_time <- window_data$time[nrow(window_data)]
            
            # Ajouter les caractéristiques à la liste
            phase_features[[length(phase_features) + 1]] <- features
        }

        # Combiner les caractéristiques de toutes les fenêtres de cette phase
        if (length(phase_features) > 0) {
            phase_features_df <- do.call(rbind, phase_features)
            
            # Ajouter au participant_features
            participant_features[[length(participant_features) + 1]] <- phase_features_df
        } else {
            cat("Aucune caractéristique extraite pour l'activité", current_activity, "phase", current_phase, "du participant", participant_id, "\n")
        }
    }

    # Après avoir traité toutes les phases d'activité pour le participant, combiner les caractéristiques
    if (length(participant_features) > 0) {
        participant_features_df <- do.call(rbind, participant_features)
        
        # Ajouter au participant_data_list
        participant_data_list[[participant_id]] <- participant_features_df
    } else {
        cat("Aucune donnée valide pour le participant", participant_id, "\n")
    }
}

# Après avoir traité tous les participants, combiner les données en un seul data frame
if (length(participant_data_list) > 0) {
    all_features_df <- do.call(rbind, participant_data_list)
} else {
    stop("Aucune donnée n'a été extraite pour aucun participant.")
}
```

# **Sélection des caractéristiques et normalisation**

```{r}
# **Sélection des caractéristiques en minimisant la redondance**

# Séparer les étiquettes des caractéristiques
feature_data <- all_features_df[, !(names(all_features_df) %in% c("participant_id", "activity", "phase", "window_id", "start_time", "end_time"))]

# Vérifier s'il y a des colonnes de caractéristiques
if (ncol(feature_data) == 0) {
    stop("Aucune caractéristique disponible pour la sélection.")
}

# Supprimer les caractéristiques avec une variance quasi nulle
if(FALSE){
    nzv <- nearZeroVar(feature_data)
    if (length(nzv) > 0) {
        feature_data <- feature_data[, -nzv, drop = FALSE]
    }
}

# Supprimer les caractéristiques fortement corrélées
if (FALSE){
    if (ncol(feature_data) > 1) {
        corr_matrix <- cor(feature_data, use = "pairwise.complete.obs")
        high_corr <- findCorrelation(corr_matrix, cutoff = 0.9) #cutoff ajustable
        if (length(high_corr) > 0) {
            feature_data <- feature_data[, -high_corr, drop = FALSE]
        }
    }
}

# Combiner les caractéristiques sélectionnées avec les étiquettes
final_feature_set <- cbind(
    participant_id = all_features_df$participant_id,
    activity = all_features_df$activity,
    phase = all_features_df$phase,
    window_id = all_features_df$window_id,
    start_time = all_features_df$start_time,
    end_time = all_features_df$end_time,
    feature_data
)

# Exporter le jeu de données final en CSV [version 13 params = plus précis mais plus lourd]
#write.csv(final_feature_set, file = file.path(thisRES_PATH, windowSizeStr, "standardized_activity_features.csv"), row.names = FALSE)
fwrite(final_feature_set, file.path(thisRES_PATH, windowSizeStr, "standardized_activity_features.csv"))

################

# initialisation avant de selectionner les meilleures features
# Normaliser les features (only for PCA and random forest, not for output)
# We will scale the features after, in the python machine learning phase (beacause we need the same normalization for all data)
preProcValues <- preProcess(feature_data, method = c("center", "scale"))
feature_data_norm <- predict(preProcValues, feature_data)

# Appliquer la PCA pour sélectionner les meilleures features
if (TRUE) {
    pca_result <- prcomp(feature_data_norm, scale. = TRUE)
    # Calculer les contributions absolues des variables sur chaque axe de la PCA
    loadings <- abs(pca_result$rotation)
    # Déterminer le nombre de composantes principales nécessaires pour expliquer 90% de la variance
    num_components <- which(cumsum(summary(pca_result)$importance[2, ]) >= 0.9)[1]
    # Sélectionner la variable qui contribue le plus à chaque composante principale
    selected_variables <- character()
    for (i in 1:num_components) {
        max_contributor <- names(which.max(loadings[, i]))
        selected_variables <- unique(c(selected_variables, max_contributor))
    }
    cat("Variables sélectionnées :", paste(selected_variables, collapse = ", "), "\n")
    # Filtrer les données avec les variables sélectionnées (not normalized)
    feature_data_selected <- feature_data[, selected_variables, drop = FALSE]
}

# Utiliser Random Forest pour affiner la sélection des meilleures features
if (FALSE){
    # Convertir la variable activity en facteur
    all_features_df$activity <- as.factor(all_features_df$activity)
    set.seed(123)
    rf_model <- randomForest(x = feature_data_norm, y = all_features_df$activity, importance = TRUE)
    var_importance <- importance(rf_model)
    var_importance_df <- data.frame(Variable = rownames(var_importance), Importance = var_importance[, "MeanDecreaseGini"])
    var_importance_df <- var_importance_df[order(-var_importance_df$Importance), ]
    top_variables <- head(var_importance_df$Variable, 10)
    # Filtrer les données avec les variables les plus importantes
    feature_data_selected <- feature_data[, top_variables, drop = FALSE]
}

# Combiner les caractéristiques sélectionnées avec les étiquettes
final_feature_set <- cbind(
    participant_id = all_features_df$participant_id,
    activity = all_features_df$activity,
    phase = all_features_df$phase,
    window_id = all_features_df$window_id,
    start_time = all_features_df$start_time,
    end_time = all_features_df$end_time,
    feature_data_selected
)

# Exporter le jeu de données final en CSV
#write.csv(final_feature_set, file = file.path(thisRES_PATH, windowSizeStr, "standardized_activity_features_onlySelectedFeatures.csv"), row.names = FALSE)
fwrite(final_feature_set, file.path(thisRES_PATH, windowSizeStr, "standardized_activity_features_onlySelectedFeatures.csv"))
```


```{r}
#run above

# Vérifier s'il y a des valeurs NaN dans chaque colonne
#nan_check <- sapply(final_feature_set, function(x) any(is.nan(x)))
# Afficher les colonnes qui contiennent des NaN
#colnames(final_feature_set)[nan_check]
```